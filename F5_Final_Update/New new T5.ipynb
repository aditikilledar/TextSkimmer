{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TVT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since cnn_dailymail couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration '3.0.0' at C:\\Users\\TVT\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\0.0.0\\96df5e686bee6baa90b8bee7c28b81fa3fa6223d (last modified on Mon Apr 22 00:25:23 2024).\n"
     ]
    }
   ],
   "source": [
    "# Load dataset for fine-tuning (e.g., CNN/DailyMail dataset)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "small_dataset = dataset[\"train\"].select(range(100))  # Select the first 100 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset for training\n",
    "def tokenize_function(example):\n",
    "    source_text = example[\"article\"]\n",
    "    target_text = example[\"highlights\"]\n",
    "    source_tokenized = tokenizer(source_text, truncation=True, padding=\"max_length\", max_length=1024, return_tensors=\"pt\")\n",
    "    target_tokenized = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=150, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": source_tokenized.input_ids,\n",
    "        \"attention_mask\": source_tokenized.attention_mask,\n",
    "        \"labels\": target_tokenized.input_ids,\n",
    "    }\n",
    "\n",
    "tokenized_datasets = small_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TVT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1000,\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [16:55<00:00, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1015.5293, 'train_samples_per_second': 0.295, 'train_steps_per_second': 0.074, 'train_loss': 4.186572265625, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=4.186572265625, metrics={'train_runtime': 1015.5293, 'train_samples_per_second': 0.295, 'train_steps_per_second': 0.074, 'train_loss': 4.186572265625, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_t5_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "AI (AI) is a field of computer science that aims to create systems capable of performing tasks typically require human intelligence. The concept of AI dates back to ancient times, with early ideas emerging in Greek mythology and ancient Greek philosophy. However, the modern era of AI began in the mid-20th century with the development of computer technology.\n"
     ]
    }
   ],
   "source": [
    "# Example of generating summaries using the fine-tuned model\n",
    "input_text = \"\"\"Artificial intelligence (AI) is a field of computer science that aims to create systems capable of performing tasks that typically require human intelligence. \n",
    "The concept of AI dates back to ancient times, with early ideas emerging in Greek mythology and ancient Greek philosophy. However, the modern era of AI began in the mid-20th \n",
    "century with the development of computer technology and the advent of digital computing. In 1956, the term \"artificial intelligence\" was coined at the Dartmouth Conference,\n",
    " where researchers gathered to discuss the potential of creating machines that could mimic human cognitive abilities. Since then, AI has evolved rapidly, with significant \n",
    " advancements in areas such as machine learning, natural language processing, computer vision, and robotics. AI technologies have been applied across various industries, \n",
    " including healthcare, finance, transportation, and entertainment, revolutionizing the way we live and work. From virtual assistants like Siri and Alexa to self-driving cars\n",
    "   and advanced medical diagnostic systems, AI has become an integral part of our daily lives. However, AI also raises ethical and societal concerns, including issues related\n",
    "     to privacy, bias, job displacement, and the potential for misuse of AI-powered systems. Despite these challenges, the pursuit of artificial intelligence continues to drive\n",
    "       innovation and shape the future of technology and society.\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "generated_summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "generated_summary = tokenizer.decode(generated_summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\")\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 0.4402985074626865\n",
      "ROUGE-2 F1 Score: 0.4210526315789473\n",
      "ROUGE-L F1 Score: 0.4328358208955224\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "reference_summary = input_text\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(\"ROUGE-1 F1 Score:\", scores['rouge1'].fmeasure)\n",
    "print(\"ROUGE-2 F1 Score:\", scores['rouge2'].fmeasure)\n",
    "print(\"ROUGE-L F1 Score:\", scores['rougeL'].fmeasure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
