{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxpO46Ul8qp0",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "## ProjF5 - Final Model\n",
        "\n",
        "Use this document as a template to provide the evaluation of your final model. You are welcome to go in as much depth as needed.\n",
        "\n",
        "Make sure you keep the sections specified in this template, but you are welcome to add more cells with your code or explanation as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ibqBgqcr8qp3",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVrK750r8qp4",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "### 1. Load and Prepare Data\n",
        "\n",
        "This should illustrate your code for loading the dataset and the split into training, validation and testing. You can add steps like pre-processing if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSN83-HAENSQ"
      },
      "source": [
        "The dataset is already stored\n",
        "While loading the dataset, we have loaded the train dataset, using the load_dataset function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WrfkWJdyENSQ",
        "outputId": "de42242d-704c-468d-a784-9bf269bad8d4"
      },
      "outputs": [],
      "source": [
        "# Load dataset for fine-tuning (e.g., CNN/DailyMail dataset)\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "small_dataset = dataset[\"train\"].select(range(100))  # Select the first 100 examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g1hJHzVNENSR",
        "outputId": "f13b57f0-8d2f-485c-f2d8-455536b700ed"
      },
      "outputs": [],
      "source": [
        "# Load dataset for fine-tuning (e.g., CNN/DailyMail dataset)\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "small_dataset = dataset[\"train\"].select(range(100))  # Select the first 100 examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HTHsApfENSR"
      },
      "source": [
        "Import libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FU5T3yUUENSR",
        "outputId": "ca85cda9-c8b5-48ef-8304-ac72237196ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7SuZOHJENSR"
      },
      "outputs": [],
      "source": [
        "# Tokenize dataset for training\n",
        "def tokenize_function(example):\n",
        "    source_text = example[\"article\"]\n",
        "    target_text = example[\"highlights\"]\n",
        "    source_tokenized = tokenizer(source_text, truncation=True, padding=\"max_length\", max_length=1024, return_tensors=\"pt\")\n",
        "    target_tokenized = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=150, return_tensors=\"pt\")\n",
        "    return {\n",
        "        \"input_ids\": source_tokenized.input_ids,\n",
        "        \"attention_mask\": source_tokenized.attention_mask,\n",
        "        \"labels\": target_tokenized.input_ids,\n",
        "    }\n",
        "\n",
        "tokenized_datasets = small_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K3ISILq8qp5"
      },
      "source": [
        "### 2. Prepare your Final Model\n",
        "\n",
        "Here you can have your code to either train (e.g., if you are building it from scratch) your model. These steps may require you to use other packages or python files. You can just call them here. You don't have to include them in your submission. Remember that we will be looking at the saved outputs in the notebooked and we will not run the entire notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qF2FufyqENSS",
        "outputId": "5fcfac26-dac2-4f4a-9db9-9d30ea05f8bd"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9sgGLHuENSS",
        "outputId": "3c2b63d9-5326-4da3-99cd-9df0b45f0bdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\TVT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=1000,\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yiq61RskENSS",
        "outputId": "bf3fa426-0e0a-48e0-ae31-d8c27853a142"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [16:55<00:00, 13.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 1015.5293, 'train_samples_per_second': 0.295, 'train_steps_per_second': 0.074, 'train_loss': 4.186572265625, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=75, training_loss=4.186572265625, metrics={'train_runtime': 1015.5293, 'train_samples_per_second': 0.295, 'train_steps_per_second': 0.074, 'train_loss': 4.186572265625, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suRWbm8MENSS"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./fine_tuned_t5_small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofGOo3lM8qp5",
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        }
      },
      "source": [
        "### 3. Model Performance\n",
        "\n",
        "Make sure to include the following:\n",
        "\n",
        "- Performance on the training set\n",
        "- Performance on the test set\n",
        "- Provide some screenshots of your output (e.g., pictures, text output, or a histogram of predicted values in the case of tabular data). Any visualization of the predictions are welcome.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4YXhcNKENST"
      },
      "source": [
        "Example of Abstractive Summarization using the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance on Single Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pBLAwEbUENST",
        "outputId": "7555ebdc-d976-4e97-f554-c8223d7e9594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1439\n",
            "Generated Summary:\n",
            "(AI) is a field of computer science that aims to create systems capable of performing tasks that typically require human intelligence. AI is a field of computer science that aims to create systems capable of performing tasks that typically require human intelligence. AI technologies have been applied across various industries, including healthcare, finance, transportation, and entertainment.\n"
          ]
        }
      ],
      "source": [
        "# Example of generating summaries using the fine-tuned model\n",
        "input_text = \"\"\"Artificial intelligence (AI) is a field of computer science that aims to create systems capable of performing tasks that typically require human intelligence.\n",
        "The concept of AI dates back to ancient times, with early ideas emerging in Greek mythology and ancient Greek philosophy. However, the modern era of AI began in the mid-20th\n",
        "century with the development of computer technology and the advent of digital computing. In 1956, the term \"artificial intelligence\" was coined at the Dartmouth Conference,\n",
        " where researchers gathered to discuss the potential of creating machines that could mimic human cognitive abilities. Since then, AI has evolved rapidly, with significant\n",
        " advancements in areas such as machine learning, natural language processing, computer vision, and robotics. AI technologies have been applied across various industries,\n",
        " including healthcare, finance, transportation, and entertainment, revolutionizing the way we live and work. From virtual assistants like Siri and Alexa to self-driving cars\n",
        "   and advanced medical diagnostic systems, AI has become an integral part of our daily lives. However, AI also raises ethical and societal concerns, including issues related\n",
        "     to privacy, bias, job displacement, and the potential for misuse of AI-powered systems. Despite these challenges, the pursuit of artificial intelligence continues to drive\n",
        "       innovation and shape the future of technology and society.\n",
        "\"\"\"\n",
        "\n",
        "print(len(input_text))\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "generated_summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "generated_summary = tokenizer.decode(generated_summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Iaa8vkQENST",
        "outputId": "f3b85e03-8afe-4e03-d778-0d0458d62c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 F1 Score: 0.4402985074626865\n",
            "ROUGE-2 F1 Score: 0.4210526315789473\n",
            "ROUGE-L F1 Score: 0.4328358208955224\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "reference_summary = input_text\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "# Print ROUGE scores\n",
        "print(\"ROUGE-1 F1 Score:\", scores['rouge1'].fmeasure)\n",
        "print(\"ROUGE-2 F1 Score:\", scores['rouge2'].fmeasure)\n",
        "print(\"ROUGE-L F1 Score:\", scores['rougeL'].fmeasure)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm_75j_JENST"
      },
      "source": [
        "Performance on Subset of Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 F1 Score: 0.4825110004435335\n",
            "ROUGE-2 F1 Score: 0.41578939509180834\n",
            "ROUGE-L F1 Score: 0.4040728637546249\n"
          ]
        }
      ],
      "source": [
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Initialize variables to store cumulative scores\n",
        "rouge1 = 0.0\n",
        "rouge2 = 0.0\n",
        "rougeL = 0.0\n",
        "cnt = 0\n",
        "train_subset = dataset[\"test\"].select(range(500))\n",
        "\n",
        "for input_text in train_subset[\"article\"]:\n",
        "    if len(input_text) < 700:\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "        generated_summary_ids = model.generate(input_ids, max_length=150, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        generated_summary = tokenizer.decode(generated_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        scores = scorer.score(input_text, generated_summary)\n",
        "\n",
        "        cnt += 1\n",
        "        # Update cumulative scores\n",
        "        rouge1 += scores['rouge1'].fmeasure\n",
        "        rouge2 += scores['rouge2'].fmeasure\n",
        "        rougeL += scores['rougeL'].fmeasure\n",
        "\n",
        "\n",
        "# Print average ROUGE scores\n",
        "print(\"ROUGE-1 F1 Score:\", rouge1/cnt)\n",
        "print(\"ROUGE-2 F1 Score:\", rouge2/cnt)\n",
        "print(\"ROUGE-L F1 Score:\", rougeL/cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LWz6lVwGq8J"
      },
      "source": [
        "Performance on Subset of Train Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "dgH5ebpBGq8K",
        "outputId": "664b2cd9-6a5f-49ec-cb5a-50199c2bfab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 F1 Score: 0.4880952380952381\n",
            "ROUGE-2 F1 Score: 0.4758671047827674\n",
            "ROUGE-L F1 Score: 0.43452380952380953\n"
          ]
        }
      ],
      "source": [
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Initialize variables to store cumulative scores\n",
        "rouge1 = 0.0\n",
        "rouge2 = 0.0\n",
        "rougeL = 0.0\n",
        "cnt = 0\n",
        "train_subset = dataset[\"train\"].select(range(1500))\n",
        "\n",
        "for input_text in train_subset[\"article\"]:\n",
        "    if len(input_text) < 700:\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "        generated_summary_ids = model.generate(input_ids, max_length=150, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        generated_summary = tokenizer.decode(generated_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        scores = scorer.score(input_text, generated_summary)\n",
        "        cnt += 1\n",
        "        # Update cumulative scores\n",
        "        rouge1 += scores['rouge1'].fmeasure\n",
        "        rouge2 += scores['rouge2'].fmeasure\n",
        "        rougeL += scores['rougeL'].fmeasure\n",
        "\n",
        "\n",
        "# Print average ROUGE scores\n",
        "print(\"ROUGE-1 F1 Score:\", rouge1/cnt)\n",
        "print(\"ROUGE-2 F1 Score:\", rouge2/cnt)\n",
        "print(\"ROUGE-L F1 Score:\", rougeL/cnt)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
